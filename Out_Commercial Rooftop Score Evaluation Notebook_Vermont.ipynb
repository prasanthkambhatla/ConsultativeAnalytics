{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "*  [Rooftop Solution Introduction](#modelintro)\n",
    "   *  [Model Target](#modeltarget)\n",
    "   *  [In This Analysis We Will](#analysissteps) \n",
    "   *  [In this Notebook We Create](#outputlist)\n",
    "   \n",
    "   <br>\n",
    "\n",
    "*  [Explore the Input File](#exploreinput)\n",
    "   *  [Import Packages and Load the Evaluation Dataset](#import)\n",
    "   *  [Address Errors & Geocoding Hit Rate](#hitrate)\n",
    "   *  [Addresses Distribution by States](#statesdist)\n",
    "   *  [Address Distribution by Regions](#regionsdist)\n",
    "   *  [Snapdate Distribution](#snapdatedist)\n",
    "   *  [Other Attribute Distribution (year_built)](#year-built-distribution)\n",
    "\n",
    "   \n",
    "   <br>\n",
    "\n",
    "*  [Evaluate the Solution](#evaluatesolution)\n",
    "   * [Score Distribution National and Regional](#scoredist)\n",
    "   * [How to Update Regional and National Target Variables](#updatetarget)\n",
    "   * [Overall Claim Rates Stability](#over-all-claimrates)\n",
    "   * [National Gains Charts](#claimratesgainschart)\n",
    "\n",
    "\n",
    "   \n",
    "   <br>\n",
    "\n",
    "*  [Pricing Model Evaluation](#pricingmodelevaluation)\n",
    "   * [Gains Chart by old rooftop score](#oldrooftopscore)\n",
    "   \n",
    "   <br>\n",
    "\n",
    "*  [Multiple Snap Dates for Same Address](#multiple)\n",
    "   * [Gains Chart by Snapdate Year](#snapdatesgainschart)\n",
    "   * [Difference in Score Distribution between Snapdates](#scoredistbeweensnapdates)\n",
    "   \n",
    "   <br>\n",
    "\n",
    "*  [Appendix](#appendix)\n",
    "   * [Customer Facing Attributes Fill Rate](#attributesfillrate)\n",
    "   * [Hit Rate Summary](#hitratefull)\n",
    "\n",
    "   \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rooftop Solution lntroduction: <a id='modelintro'></a>\n",
    "\n",
    "LexisNexis Rooftop is a series of loss propensity models that produce a score to predict the liklihood of a future weather-related claim. It is built with two types of data: geospatially aggregated attributes and property characteristics from public records. The geospatially aggregations are on claims and publicly-available weather attributes. The solution consists of 6 different models across 4 regions across the entire United States (including AK and HI). \n",
    "\n",
    "* <b>Central Zone 1:</b> TX, OK, AR\n",
    "* <b>Central Zone 2:</b> IL, CO, NE, MO, IA, KS, NM, MT, SD, ND, WY\n",
    "* <b>South Region:</b> AL, FL, GA, KY, LA, MS, NC, SC, TN, VA\n",
    "* <b>Northeast Zone 1:</b> CT, DC, DE, IN, MA, MD, ME, MI, NH\n",
    "* <b>Northeast Zone 2:</b> NJ, NY, OH, PA, RI, VT, WI, WV\n",
    "* <b>West Region: </b> CA, AK, AZ, HI, ID, NV, OR, UT, WA\n",
    "\n",
    "The scores are calibrated between 1 and 100 so that the numerical score represents the same level of risk, regardless of which underlying model produced the result. A score of 1 represents the lowest risk of a future weather-related claim and a score of 100 represents the highest risk of a future weather-related claim. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Model Target <a id='modeltarget'></a>\n",
    " The target for each model includes open and closed claims with a claim amount over $5,000. These include both Catastrophe and Non-Catastrophe losses. The weather perils included in each model target differs based on the region. \n",
    "\n",
    "<b>Central and South Region Target</b>\n",
    " * Wind, Hail, or Weather-Water peril losses that occur in the 12 months following the date the record was scored (`snapdate`)\n",
    "\n",
    "<b> Northeast and West Region Target</b>\n",
    " * Wind, Hail, Weather-Water, Water, Freeze, and Miscellaneous peril losses that occur in the 18 months following the date the record was scored (`snapdate`)\n",
    "\n",
    "Using CLUE Commercial & Property contributions, we were able to identify properties with a claim in the 12 -18 months following the snapdate. Claims from Business Owner, Multi-Peril, Commercial Property, and Personal Property policies were considered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In This Analysis We Will: <a id='analysissteps'></a>\n",
    "\n",
    "<b>Explore the input file</b><br>\n",
    "    Before we begin understanding the model performance, we will first understand the input file's addresses and snap dates (the dates in which the record is to be scored). We will explore whether or not the addresses in the file are valid, scorable locations and which regions those locations cover. We also need to confirm the snap dates are valid. Dates prior to 2017 may have some inconsistencies and dates within the last year will not have had time for claim development. Additionally, we'll check the distribution and fill rate of any other interesting data elements within the file that we may wish to evaluate later on.\n",
    "<br><br>   \n",
    "<b>Evaluate the Solution</b><br>\n",
    "    Once we are comfortable with our input file, we will shift our focus to the actual model score. First we will look at the distribution of the score on the input addresses. Next we will show you how to update the target variable fields (`addr_future_claim_indicator` and `addr_future_claim_amount`) with carrier-sourced claims data. We will then look at the stability of the model target before evaluating the performance of the score on the input sample.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In This Notebook We Create: <a id='outputlist'></a>\n",
    "* [<b>Hit rate charts</b> to see the percentage of properties that can be scored](#hitrate)\n",
    "* [<b>State distribution chart</b> to understand which model regions are relevant](#statesdist)\n",
    "* [<b>Region distribution chart</b>](#regionsdist)\n",
    "* [<b>Snapdate distribution chart</b> to understand if some records should be excluded from the evaluation](#snapdatedist) \n",
    "* [<b>Year_built distribution chart</b> as an example to check distribution and fill rate for attributes in the file](#year-built-distribution)\n",
    "* [<b>Score distribution chart</b> to understand the segmentation and spread of the score](#scoredist)\n",
    "* [<b>Updated target variables</b> to measure score performance against](#createregionaltarget)\n",
    "* [<b>Claim rate by year chart</b> to understand if the updated target is stable enough over time to analyze](#over-all-claimrates)\n",
    "* [<b>National Claim Rate, Claim Severity, and Loss Ratio gains charts</b> to show model performance](#claimratesgainschart)\n",
    "* [Analysis highlighting significant score differences across snap dates for the same property](#multiple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "# Explore the Input File <a id='exploreinput'></a>\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages <a id='import'></a>\n",
    "First we will import the packages needed for the jupyter notebook to run. The yml file can be used to create a conda enviroment which contains all the needed packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use(\"default\")\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Load the evalution dataset <a id='load'></a>\n",
    "LexisNexis returns a file with the Rooftop score and attributes appended to the inpute file. This file contains all customer fields in addition to the Rooftop score and over 40 attributes that can be created. Because the input fields are returned back to the customer in this file, it should be possible to join this back to any internal datasets you may have. The most important field appended to the input dataset is `new_score` - the rooftop score field. \n",
    "\n",
    "In order to proceed with the evaluation, we must inport this evaluation file. We assume the evaluation file and evaluation notebook are in the same directory on your machine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import customer input file with rooftop score file,its pipe separated csv file\n",
    "df = pd.read_csv(\"./Out_scoreoutput.csv\", sep=\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Geocoding Hit Rate <a id='hitrate'></a>\n",
    "Every address must be precisely geocoded. There is a `geo_match` field that is returned as a part of address standardization with a numerical value of 0, 1, 4, or 5. A value of 0 means we can identify the coordinates to a precise point within the parcel. We require geo_match to equal \"0\" in order to ensure that our geospatially aggregated attributes are correct for the input address. If an address does not have a value of \"0\" in the field, we may still be able to return some attributes that don't require geospatial aggregation despite returning 999 for the score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# geocode quality check up,hit means good geocode, no hit means not able to geocode.\n",
    "print(\"generating geocoding hit rate\")\n",
    "df[\"geo_match_ind\"] = df.geo_match == 0\n",
    "report = df.geo_match_ind.value_counts().reset_index()\n",
    "report.columns = [\"geo_match\", \"count\"]\n",
    "report[\"rate\"] = report[\"count\"] / report[\"count\"].sum()\n",
    "report[\"geo_match\"] = report[\"geo_match\"].map({True: \"hit\", False: \"no hit\"})\n",
    "report[\"rate\"] = report[\"rate\"].apply(\"{:.2%}\".format)\n",
    "fig, ax = plt.subplots(figsize=(5, 2))\n",
    "ax.axis(\"off\")\n",
    "c = report.shape[1]\n",
    "ax.table(\n",
    "    cellText=np.vstack([report.columns, report.values]),\n",
    "    cellColours=[[\"lightgrey\"] * c] + [[\"white\"] * c] * report.shape[0],\n",
    "    bbox=[0, 0, 1, 1],\n",
    ")\n",
    "ax.set_title(\"Geocoding Hit Rate (Total: {})\".format(df.shape[0]), loc=\"left\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Address Standardization Error Rate <a id='errorrate'></a>\n",
    "In addition to precise geocoding, every address must have a valid address. Properties with a precise geo_match typically have a valid address, but there will be some properties that have a valid address despite the lack of precision on the coordinates. To understand which properties did not pass address standardization, we must take a look at the `err_stat` column that is returned in the file. If the value in this column starts with `E`, there is an error that prohibited LexisNexis from standardizing the address. All properties that fail address standardization will be returned with a rooftop score of 999. These addresses will not have any geospatial or property characteristic attributes returned. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"err_stat_ind\"] = df.err_stat.str[0] == \"E\"\n",
    "report = df.err_stat_ind.value_counts().reset_index()\n",
    "report.columns = [\"err_stat\", \"count\"]\n",
    "report[\"rate\"] = report[\"count\"] / report[\"count\"].sum()\n",
    "report[\"err_stat\"] = report[\"err_stat\"].map({True: \"error\", False: \"no error\"})\n",
    "report[\"rate\"] = report[\"rate\"].apply(\"{:.2%}\".format)\n",
    "fig, ax = plt.subplots(figsize=(5, 2))\n",
    "ax.axis(\"off\")\n",
    "c = report.shape[1]\n",
    "ax.table(\n",
    "    cellText=np.vstack([report.columns, report.values]),\n",
    "    cellColours=[[\"lightgray\"] * c] + [[\"none\"] * c] * report.shape[0],\n",
    "    bbox=[0, 0, 1, 1],\n",
    ")\n",
    "ax.set_title(\"Error Rate (Total: {})\".format(df.shape[0]), loc=\"left\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hit Rate Summary\n",
    "Combining the geocoding hit rate and address standardization error rate, we are able to get a full understanding of the number of records that were scorable, the number of records with at least some attributes, and the number of records that did not pass either geocoding or address standardization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################### overall summary###############\n",
    "print(\"generating overall summary...\")\n",
    "\n",
    "types = [\"Total # of Records\", \"Address Found\", \"Score Returned\"]\n",
    "x = []\n",
    "x.append(df.shape[0])  # total # of records\n",
    "x.append(df[df.err_stat.str[0] != \"E\"].shape[0])  # address found\n",
    "x.append(df[df.new_score < 101].shape[0])  # scores returned\n",
    "\n",
    "report = pd.DataFrame({\"Types\": types, \"# of Records\": x})\n",
    "report[\"Fill Rate\"] = report[\"# of Records\"] / report[\"# of Records\"][0]\n",
    "\n",
    "report[\"Fill Rate\"] = report[\"Fill Rate\"].apply(\"{:.2%}\".format)\n",
    "fig, ax = plt.subplots()  # figsize=(5,max(report.shape[0]/3.5, 2))\n",
    "ax.axis(\"off\")\n",
    "c = report.shape[1]\n",
    "ax.table(\n",
    "    cellText=np.vstack([report.columns, report.values]),\n",
    "    cellColours=[[\"lightgray\"] * c] + [[\"none\"] * c] * report.shape[0],\n",
    "    bbox=[0, 0, 1, 1],\n",
    ")\n",
    "ax.set_title(\"Overall Summary\", loc=\"left\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Address Distribution by State<a id='statesdist'></a>\n",
    "Because the model is region based, its important to know which regions are represented in the input file. Later in this analysis we will update the model target based on the regions. When evaluating regional performance, it will be important to consider volume of properties in that region. If there are not enough properties in a particular region, the performance may not be indicitive of what you would expect with a statistically significant sample. In this case, we recommend paying closer attention to the national performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"generating distribution by state\")\n",
    "zone1_st = [\"OK\", \"TX\", \"AR\"]\n",
    "zone2_st = [\"CO\", \"NE\", \"MO\", \"MN\", \"IA\", \"KS\", \"NM\", \"MT\", \"SD\", \"ND\", \"WY\", \"IL\"]\n",
    "south_st = [\"GA\", \"SC\", \"FL\", \"NC\", \"MS\", \"KY\", \"TN\", \"VA\", \"LA\", \"AL\"]\n",
    "west_st = [\"AK\", \"AZ\", \"CA\", \"HI\", \"ID\", \"NV\", \"OR\", \"UT\", \"WA\"]\n",
    "ne_st = [\n",
    "    \"CT\",\n",
    "    \"DC\",\n",
    "    \"DE\",\n",
    "    \"IN\",\n",
    "    \"MA\",\n",
    "    \"MD\",\n",
    "    \"ME\",\n",
    "    \"MI\",\n",
    "    \"NH\",\n",
    "    \"NJ\",\n",
    "    \"NY\",\n",
    "    \"OH\",\n",
    "    \"PA\",\n",
    "    \"RI\",\n",
    "    \"VT\",\n",
    "    \"WI\",\n",
    "    \"WV\",\n",
    "]\n",
    "model_st = zone1_st + zone2_st + south_st + ne_st + west_st\n",
    "report = df[df.state.isin(model_st)].state.value_counts().reset_index()\n",
    "report.columns = [\"statecode\", \"count\"]\n",
    "fig, ax = plt.subplots(\n",
    "    figsize=(max(report.shape[0] / 3, 5), max(report.shape[0] / 6, 3))\n",
    ")\n",
    "sns.barplot(x=\"statecode\", y=\"count\", data=report, ax=ax, color=\"gray\")\n",
    "ax.set_title(\"Distribution of Addresses\", loc=\"left\")\n",
    "ax.grid()\n",
    "plt.tight_layout()\n",
    "report = report.sort_values(by=[\"count\"], ascending=False)\n",
    "report[\"percentage\"] = report[\"count\"] / report[\"count\"].sum()\n",
    "report[\"percentage\"] = report[\"percentage\"].apply(\"{:.2%}\".format)\n",
    "fig, ax = plt.subplots(figsize=(7, max(report.shape[0] / 3, 2)))\n",
    "ax.axis(\"off\")\n",
    "c = report.shape[1]\n",
    "ax.table(\n",
    "    cellText=np.vstack([report.columns, report.values]),\n",
    "    cellColours=[[\"lightgray\"] * c] + [[\"none\"] * c] * report.shape[0],\n",
    "    bbox=[0, 0, 1, 1],\n",
    ")\n",
    "ax.set_title(\"Distribution of Addresses (Scorable States)\", loc=\"left\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Address Distribution by Region<a id='regionsdist'></a>\n",
    "In order to understand if there is sufficient volume to analyze the Rooftop solution regionally, we will take a look at the distribution of addresses by region with both a numerical chart and graph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"generating distribution by regions\")\n",
    "\n",
    "def assign_zone(state):\n",
    "    if state in zone1_st:\n",
    "        return \"Central\"\n",
    "    elif state in zone2_st:\n",
    "        return \"Central\"\n",
    "    elif state in south_st:\n",
    "        return \"South\"\n",
    "    elif state in west_st:\n",
    "        return \"West\"\n",
    "    elif state in ne_st:\n",
    "        return \"NorthEast\"\n",
    "    else:\n",
    "        return \"Unknown\"\n",
    "\n",
    "df[\"regions\"] = df[\"state\"].apply(assign_zone)\n",
    "\n",
    "report = df[\"regions\"].value_counts().reset_index()\n",
    "report.columns = [\"regions\", \"count\"]\n",
    "fig, ax = plt.subplots(\n",
    "    figsize=(max(report.shape[0] / 3, 5), max(report.shape[0] / 6, 3))\n",
    ")\n",
    "sns.barplot(x=\"regions\", y=\"count\", data=report, ax=ax, color=\"gray\")\n",
    "ax.set_title(\"Distribution of Addresses by regions\", loc=\"left\")\n",
    "ax.grid()\n",
    "plt.tight_layout()\n",
    "report = report.sort_values(by=[\"count\"], ascending=False)\n",
    "report[\"percentage\"] = report[\"count\"] / report[\"count\"].sum()\n",
    "report[\"percentage\"] = report[\"percentage\"].apply(\"{:.2%}\".format)\n",
    "fig, ax = plt.subplots(figsize=(7, max(report.shape[0] / 3, 2)))\n",
    "ax.axis(\"off\")\n",
    "c = report.shape[1]\n",
    "ax.table(\n",
    "    cellText=np.vstack([report.columns, report.values]),\n",
    "    cellColours=[[\"lightgray\"] * c] + [[\"none\"] * c] * report.shape[0],\n",
    "    bbox=[0, 0, 1, 1],\n",
    ")\n",
    "ax.set_title(\"Distribution of Addresses (Scorable States)\", loc=\"left\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Snapdate distribution <a id='snapdatedist'></a>\n",
    "\n",
    "Each record is scored as of a particular `snapdate`. Because claims history begins to thin as we consider dates further in the past, we recommend only evaluating properties with a snapdate after January 1, 2017. We also tend not to evaluate records with a snapdate within 18 months of today's date. This ensures that claims have had time to develop so that we can test if the score was preditive. This distribution will help determine if any filters need to be applied before evaluating the solution later in this analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[(df.new_score < 101) & (df.state.isin(model_st))].reset_index(\n",
    "    drop=True\n",
    ")\n",
    "print(\"generating snapdate distribution\")\n",
    "fig, ax1 = plt.subplots(figsize=(6, 3.5))\n",
    "df[\"snapyear\"] = pd.to_datetime(df.snapdate, errors=\"coerce\").dt.year\n",
    "report = df.snapyear.value_counts()\n",
    "sns.barplot(\n",
    "    x=report.index, y=report.values, ax=ax1, color=\"gray\"\n",
    ")\n",
    "ax1.set_title(\"Distribution of Snapdate\", loc=\"left\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Attribute Distribution <a id='year-built-distribution'></a>\n",
    "\n",
    "You may decide that there are additional fields you'd like to evaluate as a part of the Rooftop solution. A common field that is evaluated is `year_built`. Below is an example of how you can create distributions for other fields that are of interest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[(df.new_score < 101) & (df.state.isin(model_st))].reset_index(\n",
    "    drop=True\n",
    ")\n",
    "print(\"generating yearbuilt distribution\")\n",
    "fig, ax2 = plt.subplots(figsize=(6, 3.5))\n",
    "df[\"buildyear\"] = df.pc_yearbuilt.dropna().astype(int)\n",
    "df[\"buildyear\"] = np.maximum(df.buildyear, 1800)\n",
    "df[df.buildyear > 1800][\"buildyear\"].astype(float).hist(ax=ax2, bins=100, color=\"gray\")\n",
    "ax2.set_title(\"Distribution of Yearbuilt (>1800)\", loc=\"left\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "# Evaluate the Solution <a id='evaluatesolution'></a>\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score Distribution <a id='scoredist'></a>\n",
    "The score predicts the liklihood that a property will have a future weather-related claim in the 12 - 18 months following the snapdate. The first thing we want to test when evaluating the score is whether or not properties are segmented across the entire range of potential scores. By looking at the score distribution we can determine if scores are bunched around a single score or spread out across the range. We can also understand if the properties submitted for this test are skewed toward high or low future weather-related claim risk. The Rooftop scores increase as future weather-related risk increases, meaning a score of 100 represents a property with the highest risk of a future weather-related claim. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"generating score distribution\")\n",
    "target = \"addr_future_claim_indicator\"\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "df[\"new_score\"].astype(float).hist(ax=ax, bins=50, edgecolor=\"white\", color=\"gray\")\n",
    "ax.set_xlabel(\"score\")\n",
    "ax.set_title(\"Scores, Nationwide\", loc=\"left\")\n",
    "plt.tight_layout()\n",
    "temp1 = df[df.state.isin(zone1_st + zone2_st)]\n",
    "temp2 = df[df.state.isin(south_st)]\n",
    "temp3 = df[df.state.isin(ne_st)]\n",
    "temp4 = df[df.state.isin(west_st)]\n",
    "\n",
    "fig, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4, figsize=(20, 4.5))\n",
    "\n",
    "if temp1.shape[0] != 0:\n",
    "    temp1[\"new_score\"].astype(float).hist(\n",
    "        ax=ax1, bins=50, edgecolor=\"white\", color=\"gray\"\n",
    "    )\n",
    "    ax1.set_xlabel(\"score\")\n",
    "    ax1.set_title(\"Scores, Central Region\", loc=\"left\")\n",
    "\n",
    "if temp2.shape[0] != 0:\n",
    "    temp2[\"new_score\"].astype(float).hist(\n",
    "        ax=ax2, bins=50, edgecolor=\"white\", color=\"gray\"\n",
    "    )\n",
    "    ax2.set_xlabel(\"score\")\n",
    "    ax2.set_title(\"Scores, South Region\", loc=\"left\")\n",
    "\n",
    "if temp3.shape[0] != 0:\n",
    "    temp3[\"new_score\"].astype(float).hist(\n",
    "        ax=ax3, bins=50, edgecolor=\"white\", color=\"gray\"\n",
    "    )\n",
    "    ax3.set_xlabel(\"score\")\n",
    "    ax3.set_title(\"Scores, Northeast Region\", loc=\"left\")\n",
    "\n",
    "if temp4.shape[0] != 0:\n",
    "    temp4[\"new_score\"].astype(float).hist(\n",
    "        ax=ax4, bins=50, edgecolor=\"white\", color=\"gray\"\n",
    "    )\n",
    "    ax4.set_xlabel(\"score\")\n",
    "    ax4.set_title(\"Scores, West Region\", loc=\"left\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updating Target Variable <a id='updatetarget'></a>\n",
    "The precise target for this model varies depending on the region the property is in. In this section we will define the target more precisely and use the included loss columns in the file to instruct you on how to update the `addr_future_claim_indicator` and `addr_future_claim_amt` fields. Though our example won't actually change the target variables, it can be used as a template for a more customized analysis. The included target variable field use CLUE Commercial & Property contributions to identify pclaims in the 12 -18 months following the snapdate. Claims from Business Owner, Multi-Peril, Commercial Property, and Personal Property policies were considered. \n",
    "\n",
    "Once we've updated the target variables, we'll be able to evaluate the score's ability to predict future weather-related claims. In order to do this, we must first update the target variable field regionally and then combine the regional targets to create a \"national\" target. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Regional Targets <a id='createregionaltarget'></a>\n",
    "\n",
    "For the Central and South regional models, the target includes ***claims over $5,000 related to Wind, Hail, Weather Water perils 12 months after the snapdate***.\n",
    "\n",
    "For the Northeast and West region, the model target includes ***claims over $5,000 related to Wind, Hail, Weather Water, Water, and Freeze perils within 18 months after the snapdate***.\n",
    "\n",
    "First we will update the target variables for the Central and South Regions before updating the target variables for the Northeast and West regional models.\n",
    "\n",
    "\n",
    "\n",
    "#### Central & South Targets\n",
    "The states in the Central region are as follows: ['OK','TX','AR','CO','NE','MO','MN','IA','KS','NM','MT','SD','ND','WY','IL']\n",
    "\n",
    "The states in the South region are as follows: ['GA', 'SC', 'FL', 'NC', 'MS', 'KY', 'TN', 'VA', 'LA', 'AL']\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Northeast & West Targets\n",
    "\n",
    "  The states in the West region are as follows = ['AK', 'AZ', 'CA', 'HI', 'ID', 'NV', 'OR', 'UT', 'WA']\n",
    "\n",
    "  The states in the Northeast region are as follows   = ['CT', 'DC', 'DE', 'IN', 'MA', 'MD', 'ME', 'MI', 'NH', 'NJ', 'NY', 'OH', 'PA', 'RI', 'VT', 'WI', 'WV']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "temp_ne_west = df[df.state.isin(ne_st + west_st)]\n",
    "\n",
    "# hail, wind, and water,and other related loss columns.\n",
    "values_to_match = ['WEATHER','HAIL','WIND','FREEZE','WATER']\n",
    "loss_columns=['AMOUNT_PAID']\n",
    "#create future loss amount\n",
    "\n",
    "\n",
    "#create future loss amount\n",
    "temp_ne_west['addr_future_claim_amt_ne_west'] = temp_ne_west[temp_ne_west['summary_cause_of_loss'].isin(values_to_match)][loss_columns].apply(lambda x: x[x > 5000].sum(), axis=1)\n",
    "\n",
    "# create```addr_future_claim_indicator```\n",
    "\n",
    "temp_ne_west.loc[temp_ne_west['addr_future_claim_amt_ne_west'] > 0, 'addr_future_claim_indicator_ne_west'] = 1\n",
    "\n",
    "\n",
    "# merge back to the LexisNexis return file\n",
    "df_merge_ne_west = pd.merge(df, temp_ne_west[['uid','addr_future_claim_amt_ne_west','addr_future_claim_indicator_ne_west']], how='left', on='uid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating National Target <a id='createnationaltarget'></a>\n",
    "Once the regional targets have been updated individually, we merge the records back together for our \"national\" target. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge = df_merge_ne_west\n",
    "\n",
    "# ```addr_future_claim_indicator```\n",
    "\n",
    "df_merge[\"addr_future_claim_amt\"] = df_merge[\n",
    "    [\"addr_future_claim_amt_ne_west\"]\n",
    "].apply(lambda x: x[x > 0].sum(), axis=1)\n",
    "\n",
    "\n",
    "# create```addr_future_claim_indicator```\n",
    "\n",
    "df_merge.loc[df_merge[\"addr_future_claim_amt\"] > 0, \"addr_future_claim_indicator\"] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall Claim Rates <a id='over-all-claimrates'></a>\n",
    "\n",
    "Now that the target has been updated, we can take a look at the volatility of the target to ensure it is consistent enough to be analyzed further. Claim rates vary from year to year so the goal isn't necessarily to see perfectly stable performance, but to assess if the claim rate is sufficient enough over time to include all valid snap dates. We sometimes see more recent snap dates with less time for claim development with a much different claim rate than the other properties in the file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################### claim rate time series\n",
    "print(\"generating claim rate time series\")\n",
    "df_merge[\"snapdate\"] = pd.to_datetime(df_merge.snapdate, errors=\"coerce\")\n",
    "df_merge[\"date\"] = df_merge.snapdate.dt.to_period(\"M\")\n",
    "\n",
    "df_merge[\"addr_future_claim_indicator\"] = df_merge[\"addr_future_claim_indicator\"].fillna(0)\n",
    "\n",
    "claim_rates = (df_merge.groupby(\"date\").addr_future_claim_indicator.sum() /\n",
    "              df_merge.groupby(\"date\").addr_future_claim_indicator.count()).reset_index()\n",
    "\n",
    "claim_rates.columns = [\"date\", \"claim rate\"]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "claim_rates.plot.line(x=\"date\", y=\"claim rate\", ax=ax, color=\"gray\")\n",
    "ax.set_title(\"Claim Rate Volatility\", loc=\"left\")\n",
    "ax.set_xlabel(\"snapdate\")\n",
    "ax.set_ylabel(\"claim rate\")\n",
    "ax.yaxis.set_major_formatter(mtick.PercentFormatter(1.0))\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## National Gains Charts\n",
    "With our [updated target variables](#updatetarget), we will evaluate the predictive power of the model overall. Though the model is meant to predict claim frequency or rate, we provide gains charts for claim severity as well. If Premium is provided, we can consider the performance of Rooftop above and beyond what is already being charged by assessing Loss Ratio in our gains charts. A successful gains chart will have a good spread of properties across the bins with claim rate, severity, or loss ratio increasing as the bin values increase. <br> \n",
    "\n",
    "In order to visualize these gains charts we created a function called  `create_gains` capable of plotting claim rate, claim severity, and loss ratio gains charts.\n",
    "\n",
    "* `df` is the dataframe name which is used to generate gains chart.\n",
    "* `title` takes a string as the plot title name.\n",
    "* `x_label` takes a string as the x axis label name.\n",
    "* `y_label` takes a string as the left side y axis name (eg.exposure).\n",
    "* `y_label2` takes a string as the right side y axis name.\n",
    "* `claim_rates=True` indicates to the function to plot claim rates gains chart.\n",
    "* `loss_ratio=True` indicates to the function to plot loss ratio gains chart.\n",
    "* `claim_severity=True` indicates to function to plot claim severity gains chart.\n",
    "* `cuts` controls the number of bins in the gains chart. In this notebook, we used 5, 10, 20 bins.\n",
    "* `target` column is the field with the model target. The default is `addr_future_claim_indicator`, which was generated earlier in this notebook.\n",
    "* `future_loss` column is the field with the total loss dollars associated with target claims. The default is `addr_future_claim_amt` column which was also generated earlier in this notebook.\n",
    "* `earned_premium` points to the premium column name as the denominator for the loss_ratio equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a plot gains chart function for plotting purposes.\n",
    "def create_gains(\n",
    "    ax=None,\n",
    "    df=None,\n",
    "    title=None,\n",
    "    x_label=None,\n",
    "    y_label=None,\n",
    "    y_label2=None,\n",
    "    claim_rates=None,\n",
    "    loss_ratio=None,\n",
    "    claim_severity=None,\n",
    "    cuts=None,\n",
    "    target=\"addr_future_claim_indicator\",\n",
    "    future_loss=\"addr_future_claim_amt\",\n",
    "    earned_premium=None,\n",
    "):\n",
    "    ax1 = ax\n",
    "    fig, ax1 = plt.subplots(figsize=(6, 4))\n",
    "    ax2 = ax1.twinx()\n",
    "    df[\"exposure\"] = 1\n",
    "    df[\"bins\"] = pd.cut(df[\"new_score\"], cuts)\n",
    "    if claim_rates is not None:\n",
    "        gains = df.groupby(\"bins\").agg({\"exposure\": \"sum\", target: \"sum\"}).reset_index()\n",
    "        gains[\"claim_rate\"] = gains[target] / gains[\"exposure\"]\n",
    "        avg_claim_rate = round(df[target].sum() / df.shape[0] * 100, 2)\n",
    "        gains[\"claim_rate\"].plot(\n",
    "            kind=\"line\", c=\"black\", lw=2, ls=\"--\", marker=\"o\", ax=ax2\n",
    "        )  # c='red'\n",
    "        gains = gains.reset_index()\n",
    "        ax1.set_xlabel(\"Average Claim Rate: {}%\".format(avg_claim_rate))\n",
    "        if y_label2 is not None:\n",
    "            ax2.set_ylabel(y_label2)\n",
    "            ax2.yaxis.set_major_formatter(mtick.PercentFormatter(1))\n",
    "\n",
    "    if claim_severity is not None:\n",
    "        gains = (\n",
    "            df.groupby(\"bins\")\n",
    "            .agg({future_loss: \"sum\", \"exposure\": \"sum\", target: \"sum\"})\n",
    "            .reset_index()\n",
    "        )\n",
    "        gains[\"claim_severity\"] = gains[future_loss] / gains[target]\n",
    "        gains[\"claim_severity\"].plot(\n",
    "            kind=\"line\", c=\"black\", lw=2, ls=\"--\", marker=\"o\", ax=ax2\n",
    "        )  # c='red'\n",
    "        if y_label2 is not None:\n",
    "            ax2.set_ylabel(y_label2)\n",
    "            ax2.yaxis.set_major_formatter(mtick.StrMethodFormatter(\"${x:,.2f}\"))\n",
    "        if x_label is not None:\n",
    "            ax1.set_xlabel(x_label)\n",
    "\n",
    "    if loss_ratio is not None:\n",
    "        gains = (\n",
    "            df.groupby(\"bins\")\n",
    "            .agg({future_loss: \"mean\", \"exposure\": \"sum\", earned_premium: \"mean\"})\n",
    "            .reset_index()\n",
    "        )\n",
    "        gains[\"loss_ratio\"] = gains[future_loss] / gains[earned_premium]\n",
    "        gains[\"loss_ratio\"].plot(\n",
    "            kind=\"line\", c=\"black\", lw=2, ls=\"--\", marker=\"o\", ax=ax2\n",
    "        )  # c='red'\n",
    "        if y_label2 is not None:\n",
    "            ax2.set_ylabel(y_label2)\n",
    "            ax2.yaxis.set_major_formatter(mtick.StrMethodFormatter(\"${x:,.2f}\"))\n",
    "        if x_label is not None:\n",
    "            ax1.set_xlabel(x_label)\n",
    "\n",
    "    gains[\"exposure\"].plot(\n",
    "        kind=\"bar\", ax=ax1, color=\"gray\", width=0.9\n",
    "    )  # color='skyblue'\n",
    "    ax1.set_xticklabels(gains[\"bins\"], rotation=80)\n",
    "    ax1.set_ylim([0, gains[\"exposure\"].max() * 2.5])\n",
    "    ax1.tick_params(labelbottom=True)\n",
    "    ax1.xaxis.grid()\n",
    "    ax2.grid()\n",
    "    ax1.set_xticklabels(gains[\"bins\"], rotation=80)\n",
    "    if title is not None:\n",
    "        ax1.set_title(title, loc=\"left\")\n",
    "    if y_label is not None:\n",
    "        ax1.set_ylabel(y_label)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### National Claim Rate Gains Chart <a id='claimratesgainschart'></a>\n",
    "The Rooftop models are all trained to segment properties on their liklihood of a future weather-related claim. A successful rooftop test will show the claim rate increasing as the rooftop score also increases. In this section we will create several gains charts to test this. Each gains chart will have Rooftop score bins along the x-axis. The scores will be grouped in 5 bins, 10 bins, and 20 bins. There will be two y-axes: One for the bars that show the percentage of properties in each bin and another for the line that shows claim rate for the properties in each bin. The claims in each bin are based on the [updated target variables](#updatetarget) created in the earlier section. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_gains(\n",
    "    df=df_merge,\n",
    "    title=\"national_claim_rate\",\n",
    "    x_label=\"Rooftop_Score\",\n",
    "    y_label=\"exposure\",\n",
    "    y_label2=\"claim_rate\",\n",
    "    claim_rates=True,\n",
    "    cuts=np.arange(0, 120, 20),\n",
    "    target=\"addr_future_claim_indicator\",\n",
    ")\n",
    "\n",
    "create_gains(\n",
    "    df=df_merge,\n",
    "    title=\"national_claim_rate\",\n",
    "    x_label=\"Rooftop_Score\",\n",
    "    y_label=\"exposure\",\n",
    "    y_label2=\"claim_rate\",\n",
    "    claim_rates=True,\n",
    "    cuts=np.arange(0, 110, 10),\n",
    "    target=\"addr_future_claim_indicator\",\n",
    ")\n",
    "\n",
    "create_gains(\n",
    "    df=df_merge,\n",
    "    title=\"national_claim_rate\",\n",
    "    x_label=\"Rooftop_Score\",\n",
    "    y_label=\"exposure\",\n",
    "    y_label2=\"claim_rate\",\n",
    "    claim_rates=True,\n",
    "    cuts=np.arange(0, 105, 5),\n",
    "    target=\"addr_future_claim_indicator\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### National Claim Severity Gains Chart <a id='claimseveritygainschart'></a>\n",
    "Though the Rooftop models are not trained on loss amount or severity, it's still important to check if there is an unexpected relationship between the model score and loss amounts. To explore the claim severity we will create a gains chart with the Rooftop score separated into 10 bins along the x-axis. The gains chart will have two y-axes: One for the bars that  show the percentage of properties in each bin and another for the line that shows the average claim severity for the properties in each bin. If there is a relationship between claim severity and Rooftop score, we will see severity increase as the score increases. The claims in each bin are based on the [updated target variables](#updatetarget) created in the earlier section. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use claim amount to calculate claim severity\n",
    "# 10 bins\n",
    "create_gains(\n",
    "    df=df_merge,\n",
    "    title=\"national_claim_severity\",\n",
    "    x_label=\"Rooftop_Score\",\n",
    "    y_label=\"exposure\",\n",
    "    y_label2=\"claim_severity\",\n",
    "    claim_severity=True,\n",
    "    cuts=np.arange(0, 110, 10),\n",
    "    target=\"addr_future_claim_indicator\",\n",
    "    future_loss=\"addr_future_claim_amt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Multiple snap dates for same address <a id='multiple'></a>\n",
    "The input file contained multiple snap dates for the same address. To understand the model performance over time we will take a look at the claim rate gains chart by snap date year. We will also identify the properties that had a significant change in Rooftop score (a difference of 10 or more points) for potential further investigation. \n",
    "\n",
    "\n",
    "## Claim Rate Gains Charts by Snap Date Year  <a id='snapdatesgainschart'></a>\n",
    "In this section of the analysis we will filter each of our [National Claim Rate Gains Charts](#claimratesgainschart) by the snap date year. Here we want to see that the performance of the Rooftop score is stable over time (especially as we get closer to current day). For each snap date year, a successful test will show the claim rate increasing as the rooftop score also increases. Each gains chart will have Rooftop score grouped into 10 bins along the x-axis. There will be two y-axes: One for the bars that show the percentage of properties in each bin and another for the line that shows claim rate for the properties in each bin. The claims in each bin are based on the [updated target variables](#updatetarget) created in the earlier section. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performance by year\n",
    "Years = sorted(df.snapyear.unique())\n",
    "for year in Years:\n",
    "    temp = df_merge[df_merge.snapyear == year]\n",
    "    if temp[target].sum() != 0:\n",
    "        create_gains(\n",
    "            df=temp,\n",
    "            title=\"claim_rate_by_year_{}\".format(year),\n",
    "            x_label=\"Rooftop_Score\",\n",
    "            y_label=\"exposure\",\n",
    "            y_label2=\"claim_rate\",\n",
    "            claim_rates=True,\n",
    "            cuts=np.arange(0, 110, 10),\n",
    "            target=\"addr_future_claim_indicator\",\n",
    "            future_loss=\"addr_future_claim_amt\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Difference in Score Distribution <a id='scoredistbeweensnapdates'></a> \n",
    "\n",
    "Further investigation may be desired for specific properties whose Rooftop score significantly changed between snap dates. In this section we will identify the properties with multiple snap dates and measure the difference in score between them. We will then identify the properties with a significant change in score (defined as a change of 10 points or more). To visualize this we create a distribtion graph of the score differences. \n",
    "\n",
    "* Based on below analysis, across multiple snapdate for same addresses\n",
    "\n",
    "   * The largest score drop or raise between snap dates is 85.00\n",
    "   \n",
    "   * The percentage of the score drop or raise (more than 10 points) between snap dates is 14.46%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the min max score difference group by address tract key\n",
    "df = df[df.new_score < 101]\n",
    "score_diff = (\n",
    "    df.groupby([\"addr_tract_key\"])\n",
    "    .agg({\"new_score\": [\"min\", \"max\", lambda x: max(x) - min(x)]})\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "score_diff.columns = [\"addr_tract_key\", \"min_score\", \"max_score\", \"score_diff\"]\n",
    "\n",
    "# plot score difference for all the addresses\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "score_diff[\"score_diff\"].astype(float).hist(\n",
    "    ax=ax, bins=50, edgecolor=\"white\", color=\"gray\"\n",
    ")\n",
    "ax.set_xlabel(\"score difference\")\n",
    "ax.set_title(\"Scores Difference across multiple snapdate for same address\", loc=\"left\")\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "# bin the score different, and plot by score difference bins\n",
    "score_diff[\"score_cuts\"] = pd.cut(score_diff.score_diff, [-np.inf, 1, 10, 15, 50, 80])\n",
    "score_diff[\"exposure\"] = 1\n",
    "tab = score_diff.groupby(\"score_cuts\").agg({\"exposure\": \"sum\"}).reset_index()\n",
    "fig, ax1 = plt.subplots(figsize=(8, 5))\n",
    "ax2 = ax1.twinx()\n",
    "tab[\"exposure\"].plot(kind=\"bar\", ax=ax1, color=\"gray\")  # color='skyblue'\n",
    "ax1.set_xticklabels(tab.score_cuts, rotation=60)\n",
    "ax1.set_ylim([0, tab[\"exposure\"].max() * 2.5])\n",
    "ax1.set_ylabel(\"exposure\")\n",
    "ax1.set_title(\"Score difference with multiple snapdate for same address\", loc=\"left\")\n",
    "ax1.xaxis.grid()\n",
    "for ax in (ax1, ax2):\n",
    "    ax.tick_params(right=False, labelright=False)\n",
    "ax1.set_ylabel(\"exposure\")\n",
    "\n",
    "ax2.grid()\n",
    "plt.tight_layout()\n",
    "\n",
    "diff_percentage_greater_than_0 = (score_diff[\"score_diff\"] > 0).mean() * 100\n",
    "diff_percentage_greater_than_10 = (score_diff[\"score_diff\"] >= 10).mean() * 100\n",
    "print(\n",
    "    \"The largest score drop or raise between snap dates is {:.2f}\".format(\n",
    "        score_diff.score_diff.max()\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    \"The percentage of score drop or raise (more than 0 points) between snap dates is {:.2f}%\".format(\n",
    "        diff_percentage_greater_than_0\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    \"The percentage of score drop or raise (more than 10 points) between snap dates is {:.2f}%\".format(\n",
    "        diff_percentage_greater_than_10\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Pricing Model Evaluation <a id='pricingmodelevaluation'></a>\n",
    "\n",
    "space holder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gains chart by old rooftop score <a id='oldrooftopscore'></a>\n",
    "In this section of the analysis we will see the national gains chart by the old rooftop score as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a plot gains chart function for plotting purposes.\n",
    "def create_gains(\n",
    "    ax=None,\n",
    "    df=None,\n",
    "    title=None,\n",
    "    x_label=None,\n",
    "    y_label=None,\n",
    "    y_label2=None,\n",
    "    claim_rates=None,\n",
    "    cuts=None,\n",
    "    target=\"addr_future_claim_indicator\",\n",
    "):\n",
    "    ax1 = ax\n",
    "    fig, ax1 = plt.subplots(figsize=(6, 4))\n",
    "    ax2 = ax1.twinx()\n",
    "    df[\"exposure\"] = 1\n",
    "    df[\"bins\"] = pd.cut(df[\"score_RTGEOV01\"], cuts)\n",
    "    if claim_rates is not None:\n",
    "        gains = df.groupby(\"bins\").agg({\"exposure\": \"sum\", target: \"sum\"}).reset_index()\n",
    "        gains[\"claim_rate\"] = gains[target] / gains[\"exposure\"]\n",
    "        avg_claim_rate = round(df[target].sum() / df.shape[0] * 100, 2)\n",
    "        gains[\"claim_rate\"].plot(\n",
    "            kind=\"line\", c=\"black\", lw=2, ls=\"--\", marker=\"o\", ax=ax2\n",
    "        )  # c='red'\n",
    "        gains = gains.reset_index()\n",
    "        ax1.set_xlabel(\"Average Claim Rate: {}%\".format(avg_claim_rate))\n",
    "        if y_label2 is not None:\n",
    "            ax2.set_ylabel(y_label2)\n",
    "            ax2.yaxis.set_major_formatter(mtick.PercentFormatter(1))\n",
    "\n",
    "    gains[\"exposure\"].plot(\n",
    "        kind=\"bar\", ax=ax1, color=\"gray\", width=0.9\n",
    "    )  # color='skyblue'\n",
    "    ax1.set_xticklabels(gains[\"bins\"], rotation=80)\n",
    "    ax1.set_ylim([0, gains[\"exposure\"].max() * 2.5])\n",
    "    ax1.tick_params(labelbottom=True)\n",
    "    ax1.xaxis.grid()\n",
    "    ax2.grid()\n",
    "    ax1.set_xticklabels(gains[\"bins\"], rotation=80)\n",
    "    if title is not None:\n",
    "        ax1.set_title(title, loc=\"left\")\n",
    "    if y_label is not None:\n",
    "        ax1.set_ylabel(y_label)\n",
    "    plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_gains(\n",
    "    df=df_merge,\n",
    "    title=\"national_claim_rate\",\n",
    "    x_label=\"Old_Rooftop_Score\",\n",
    "    y_label=\"exposure\",\n",
    "    y_label2=\"claim_rate\",\n",
    "    claim_rates=True,\n",
    "    cuts=np.arange(0, 120, 20),\n",
    "    target=\"addr_future_claim_indicator\",\n",
    ")\n",
    "\n",
    "create_gains(\n",
    "    df=df_merge,\n",
    "    title=\"national_claim_rate\",\n",
    "    x_label=\"Old_Rooftop_Score\",\n",
    "    y_label=\"exposure\",\n",
    "    y_label2=\"claim_rate\",\n",
    "    claim_rates=True,\n",
    "    cuts=np.arange(0, 110, 10),\n",
    "    target=\"addr_future_claim_indicator\",\n",
    ")\n",
    "\n",
    "create_gains(\n",
    "    df=df_merge,\n",
    "    title=\"national_claim_rate\",\n",
    "    x_label=\"Old_Rooftop_Score\",\n",
    "    y_label=\"exposure\",\n",
    "    y_label2=\"claim_rate\",\n",
    "    claim_rates=True,\n",
    "    cuts=np.arange(0, 105, 5),\n",
    "    target=\"addr_future_claim_indicator\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix <a id='appendix'></a>\n",
    "## Customer Facing Attributes Fill Rate  <a id='attributesfillrate'></a>\n",
    "In addition to the Rooftop score, the return file comes with additional attributes. These attribute may be useful inputs to a pricing or underwriting process. In this section we will determine the fill rate of the customer-facing attributes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_layout = {\n",
    "    \"pc_roofing_material\": str,\n",
    "    \"pc_yearbuilt\": int,\n",
    "    \"pc_approx_roofareasf\": float,\n",
    "    \"pc_roof_type\": str,\n",
    "    \"years_since_known_weather_loss_5k\": str,\n",
    "    \"nearby_claim_cnt_hail_1y\": float,\n",
    "    \"nearby_claim_cnt_wind_1y\": float,\n",
    "    \"nearby_claim_cnt_wwater_1y\": float,\n",
    "    \"hail_claim_cluster_10y\": float,\n",
    "    \"auto_claim_cluster_3y\": float,\n",
    "    \"region_hail_frequency\": float,\n",
    "    \"region_wind_frequency\": float,\n",
    "    \"cnt_hail_1_year\": float,\n",
    "    \"cnt_hail_10_year\": float,\n",
    "    \"cnt_wind_1_year\": float,\n",
    "    \"cnt_wind_10_year\": float,\n",
    "    \"cnt_wind_180_days\": float,\n",
    "    \"landcover\": str,\n",
    "    \"recent_hail_event_date\": int,\n",
    "    \"recent_wind_event_date\": int,\n",
    "    \"total_market_value\": float,\n",
    "    \"new_score\": int,\n",
    "    \"nearby_claim_cnt_water_1y\": int,\n",
    "    \"nearby_claim_cnt_freeze_1y\": int,\n",
    "    \"water_claim_cluster_10y\": float,\n",
    "    \"region_water_frequency\": float,\n",
    "    \"daily_avg_temp_fahrenheit\": float,\n",
    "    \"distance_greatlake_miles\": float,\n",
    "    \"distance_coast_miles\": float,\n",
    "    \"area_landform_local_relief\": float,\n",
    "    \"area_landform_slope\": str,\n",
    "    \"area_landform_profile_type\": str,\n",
    "    \"nearby_ice_cnt_1y\": int,\n",
    "    \"nearby_ice_cnt_10y\": int,\n",
    "    \"nearby_cold_cnt_1y\": int,\n",
    "    \"nearby_cold_cnt_10y\": int,\n",
    "    \"nearby_winter_cnt_1y\": int,\n",
    "    \"nearby_winter_cnt_10y\": int,\n",
    "    \"nearby_snow_cnt_1y\": int,\n",
    "    \"nearby_snow_cnt_10y\": int,\n",
    "    \"wthr_max_temp_summer\": float,\n",
    "    \"wthr_max_temp_fall\": float,\n",
    "    \"wthr_max_temp_spring\": float,\n",
    "    \"wthr_min_temp_summer\": float,\n",
    "    \"wthr_avg_temp_1y\": float,\n",
    "    \"wthr_avg_sum_temp_1y\": float,\n",
    "    \"wthr_avg_prcp_1y\": float,\n",
    "    \"wthr_cnt_temp_b_fz_10y\": float,\n",
    "    \"wthr_cnt_temp_min_max_fz_1y\": float,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read customer attributes to a new df\n",
    "df = pd.read_csv(\"./Out_scoreoutput.csv\", sep=\"|\")\n",
    "df_customer = df[customer_layout.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################### fill rate (customer facing attributes)\n",
    "fill_counts = []\n",
    "\n",
    "print(\"generating attribute fill rate for customer use\")\n",
    "\n",
    "fill_rates = []\n",
    "for attr in df_customer:\n",
    "    if attr == \"new_score\":\n",
    "        cnt = df_customer[\n",
    "            (df_customer[attr].isna())\n",
    "            | (df_customer[attr] < 0)\n",
    "            | (df_customer[attr] > 100)\n",
    "        ].shape[0]\n",
    "        fill_counts.append(df_customer.shape[0] - cnt)\n",
    "        fill_rates.append(1 - cnt / df_customer.shape[0])\n",
    "    else:\n",
    "        if df_customer[attr].dtype.name == \"object\":\n",
    "            cnt = df_customer[\n",
    "                (df_customer[attr].isna())\n",
    "                | (df_customer[attr].astype(str).isin([\"-1.0\", \"-1\"]))\n",
    "            ].shape[0]\n",
    "            fill_counts.append(df_customer.shape[0] - cnt)\n",
    "            fill_rates.append(1 - cnt / df_customer.shape[0])\n",
    "        else:\n",
    "            cnt = df_customer[\n",
    "                (df_customer[attr].isna()) | (df_customer[attr].astype(float) == -1)\n",
    "            ].shape[0]\n",
    "            fill_counts.append(df_customer.shape[0] - cnt)\n",
    "            fill_rates.append(1 - cnt / df_customer.shape[0])\n",
    "\n",
    "report1 = pd.DataFrame({\"attribute\": df_customer.columns, \"fill rate\": fill_rates})\n",
    "report1 = report1.sort_values(by=[\"attribute\"])\n",
    "report1[\"fill rate\"] = report1[\"fill rate\"].apply(\"{:.2%}\".format)\n",
    "fig, ax = plt.subplots(figsize=(7, report1.shape[0] / 3))\n",
    "ax.axis(\"off\")\n",
    "c = report1.shape[1]\n",
    "ax.table(\n",
    "    cellText=np.vstack([report1.columns, report1.values]),\n",
    "    cellColours=[[\"lightgray\"] * c] + [[\"none\"] * c] * report1.shape[0],\n",
    "    bbox=[0, 0, 1, 1],\n",
    ")\n",
    "ax.set_title(\"Attribute Fill Rate\", loc=\"left\")\n",
    "plt.tight_layout()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hit Rate Summary<a id=\"hitratefull\"></a>\n",
    "Using the returned customer-facing attributes, we can update the Hit Rate Summary to include the number of records with at least one appended attribute. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################### overall summary###############\n",
    "print(\"generating overall summary...\")\n",
    "\n",
    "types = [\"Total # of Records\", \"Address Found\", \"Score Returned\", \"Attributes Returned\"]\n",
    "x = []\n",
    "x.append(df.shape[0])  # total # of records\n",
    "x.append(df[df.err_stat.str[0] != \"E\"].shape[0])  # address found\n",
    "x.append(df[df.new_score < 101].shape[0])  # scores returned\n",
    "x.append(max(fill_counts)) # attributes returned\n",
    "\n",
    "report = pd.DataFrame({\"Types\": types, \"# of Records\": x})\n",
    "report[\"Fill Rate\"] = report[\"# of Records\"] / report[\"# of Records\"][0]\n",
    "\n",
    "report[\"Fill Rate\"] = report[\"Fill Rate\"].apply(\"{:.2%}\".format)\n",
    "fig, ax = plt.subplots()  # figsize=(5,max(report.shape[0]/3.5, 2))\n",
    "ax.axis(\"off\")\n",
    "c = report.shape[1]\n",
    "ax.table(\n",
    "    cellText=np.vstack([report.columns, report.values]),\n",
    "    cellColours=[[\"lightgray\"] * c] + [[\"none\"] * c] * report.shape[0],\n",
    "    bbox=[0, 0, 1, 1],\n",
    ")\n",
    "ax.set_title(\"Overall Summary\", loc=\"left\")\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
